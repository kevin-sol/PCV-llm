1.更新统计时没有 质量切换

2. controller，质量自适应只关注当前gof决策，未考虑带宽或fov长期趋势，sample方法


3.controller

批量大小问题
在 train_step() 中调用了策略网络的 forward()，
而策略网络内部（位于 rl_policy.py）有断言 assert states.shape[0] == 1，以避免 CUDA 内存溢出。
如果你希望使用大于 1 的批量进行训练，需要统一解除这一限制（例如修改模型内部使其支持批量训练），
否则建议将训练过程的 batch_size 设置为 1。

计算交叉熵损失的循环
在计算质量级别损失时使用了多重循环来对每个 tile 计算交叉熵，这
部分在批量训练时可能会影响速度。建议考虑将这个过程向量化或利用批量操作函数，以提高计算效率。

梯度裁剪
目前针对 self.policy.modules_except_plm.parameters() 和低秩适应模块参数分别进行了梯度裁剪。
建议根据实际训练表现对裁剪阈值（这里设为 1.0）进行调优，确保训练过程更加稳定。

经验缓冲区管理
代码将每个 episode 的经验存入 buffer_exp 列表，再按批次随机采样进行训练。
可以考虑对经验缓冲区设定上限，并在溢出后进行淘汰（如先入先出）处理，以防止内存占用过高。

4. rl_policy
批量限制
当前 forward 方法中断言了输入 batch 大小必须为 1（assert states.shape[0] == 1, '批量大小应为1以避免CUDA内存溢出'），
这虽然可以避免 CUDA 内存溢出，但也限制了批量训练。如果资源允许，建议改造模型以支持批量训练，从而加速训练过程。

嵌入拼接与序列截断
在 forward 方法中，通过循环将当前时间步对应的回报、状态、tile 选择和质量嵌入拼接为一个序列，再执行：
stacked_inputs = stacked_inputs[:, -self.plm_embed_size:, :]
这里用 self.plm_embed_size（通常理解为隐藏维度）作为截断的长度可能会导致：
如果生成的 token 数量远大于或小于预期，这里可能截断掉关键信息或不足以表达全部信息。
建议确认是否真的希望用 PLM 的嵌入维度作为序列长度截断参数，如果是序列长度限制，
通常应采用专门的 max_seq_length 参数。

动作嵌入位置计算
根据循环逻辑，每个时间步拼接了 4 个向量（回报、状态、tile、质量），通过计算 (i+1)*4-2 与 (i+1)*4-1 得到 tile 和质量嵌入在序列中的位置，这一做法在定义嵌入结构时能够体现层次关系。但需要注意：
后续在通过 numpy 数组索引 logits 时，这里生成的索引最好转换成 PyTorch 的 tensor（例如：torch.tensor(action_embed_positions[:, 0], device=self.device)）以确保兼容性和避免潜在类型问题。